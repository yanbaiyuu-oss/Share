import akshare as ak
import pandas as pd
from datetime import datetime, timedelta
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
from typing import Callable, Dict, Any, List
import pandas_ta as ta
import numpy as np
import xlsxwriter

# 忽略 pandas 的 SettingWithCopyWarning
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)


# ==============================================================================
# 工具函数
# ==============================================================================
def format_stock_code(code: str) -> str:
    """根据股票代码的开头数字，添加SH或SZ前缀。"""
    code_str = str(code).zfill(6)
    if code_str.startswith('6'):
        return 'sh' + code_str
    elif code_str.startswith(('0', '3')):
        return 'sz' + code_str
    elif code_str.startswith(('4', '8')):
        return 'bj' + code_str
    return code_str


# ==============================================================================
# 配置类
# ==============================================================================
class Config:
    """
    程序配置类，用于管理路径、重试次数等全局设置。
    """

    def __init__(self):
        self.HOME_DIRECTORY = os.path.expanduser('~')
        self.SAVE_DIRECTORY = os.path.join(self.HOME_DIRECTORY, 'Downloads', 'CoreNews_Reports')
        self.TEMP_DATA_DIRECTORY = os.path.join(self.SAVE_DIRECTORY, 'ShareData')
        self.DATA_FETCH_RETRIES = 5
        self.DATA_FETCH_DELAY = 10
        self.MAX_WORKERS = 16


# ==============================================================================
# 数据获取类
# ==============================================================================
class DataFetcher:
    """
    负责从 Akshare 获取数据，并实现缓存和并行下载功能。
    """

    def __init__(self, config: Config):
        self.config = config
        self.today_str = datetime.now().strftime("%Y%m%d")
        self.executor = ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS)
        os.makedirs(self.config.TEMP_DATA_DIRECTORY, exist_ok=True)
        self.macd_cache_file = os.path.join(self.config.TEMP_DATA_DIRECTORY, 'MACD_hist_data_cache.txt')

    def get_file_path(self, base_name: str, is_cleaned: bool = False) -> str:
        """根据基础文件名和当前日期生成完整的文件路径。如果 is_cleaned 为 True，则添加 '_经清洗' 后缀。"""
        suffix = "_经清洗" if is_cleaned else ""
        file_name = f"{base_name}{suffix}_{self.today_str}.txt"
        return os.path.join(self.config.TEMP_DATA_DIRECTORY, file_name)

    def load_data_from_txt(self, file_path: str) -> pd.DataFrame:
        """从 | 分隔的 TXT 文件加载数据。"""
        if os.path.exists(file_path):
            try:
                # 尝试加载数据
                df = pd.read_csv(file_path, sep='|', encoding='utf-8', dtype={'股票代码': str})
                return df
            except Exception as e:
                print(f"[WARN] 错误：加载临时文件 {os.path.basename(file_path)} 失败: {e}，将重新获取。")
        return pd.DataFrame()

    def save_data_to_txt(self, df: pd.DataFrame, file_path: str):
        """将 DataFrame 保存到 | 分隔的 TXT 文件。"""
        try:
            df.to_csv(file_path, sep='|', index=False, encoding='utf-8')
            print(f"数据已保存到临时文件: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"[ERROR] 错误：保存数据到临时文件 {os.path.basename(file_path)} 失败: {e}")

    def fetch_with_cache(self, fetch_func: Callable, file_base_name: str, **kwargs: Any) -> pd.DataFrame:
        """带缓存和重试功能的数据获取函数，优先检查已清洗缓存。"""

        # 1. 优先检查是否存在已清洗的缓存文件
        cleaned_file_path = self.get_file_path(file_base_name, is_cleaned=True)
        cached_cleaned_df = self.load_data_from_txt(cleaned_file_path)
        if not cached_cleaned_df.empty:
            print(f"发现已清洗缓存文件: {os.path.basename(cleaned_file_path)}，直接加载数据。")
            return cached_cleaned_df

        # 2. 检查是否存在未清洗的缓存文件
        raw_file_path = self.get_file_path(file_base_name, is_cleaned=False)
        cached_raw_df = self.load_data_from_txt(raw_file_path)
        if not cached_raw_df.empty:
            print(f"发现原始临时文件: {os.path.basename(raw_file_path)}，直接加载数据。")
            return cached_raw_df  # 返回未清洗的，让 processor 重新清洗并保存/删除

        # 3. 如果都没有，则进行 API 抓取
        for i in range(self.config.DATA_FETCH_RETRIES):
            try:
                print(f"正在尝试第 {i + 1}/{self.config.DATA_FETCH_RETRIES} 次获取数据: {file_base_name}...")
                df = fetch_func(**kwargs)
                if df is not None and not df.empty:
                    print("数据获取成功。")
                    self.save_data_to_txt(df, raw_file_path)  # 保存原始数据
                    return df
                else:
                    print("[WARN] 数据返回为空或无效，将重试。")
                    time.sleep(self.config.DATA_FETCH_DELAY)
            except Exception as e:
                print(f"[ERROR] 获取数据时出错: {e}，将在 {self.config.DATA_FETCH_DELAY} 秒后重试。")
                time.sleep(self.config.DATA_FETCH_DELAY)
        print(f"[ERROR] 所有重试均失败，将返回空 DataFrame: {file_base_name}")
        return pd.DataFrame()

    def get_top_industry_stocks(self) -> pd.DataFrame:
        """
        获取涨跌幅前五的板块及其成分股，并进行规范化处理。
        """
        print("\n>>> 正在获取东方财富-沪深京板块-行业板块数据...")
        # 1. 获取行业板块名称列表
        # 注意：这里获取的行业板块名称列表不涉及清洗（ST过滤等），因此不应该调用 clean_data
        all_industries_df = self.fetch_with_cache(ak.stock_board_industry_name_em, '行业板块名称')
        if all_industries_df.empty:
            print("警告：未能获取行业板块名称列表，无法获取成分股。")
            return pd.DataFrame()
        top_industries = all_industries_df.sort_values(by='涨跌幅', ascending=False).head(10)
        if top_industries.empty:
            print("警告：未能找到涨幅前的板块。")
            return pd.DataFrame()
        print(f"  - 涨跌幅前排板块是: {top_industries['板块名称'].tolist()}")
        all_constituents = []
        with ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS) as executor:
            future_to_industry = {
                executor.submit(
                    self.fetch_with_cache,
                    ak.stock_board_industry_cons_em,
                    f"板块成分股_{row['板块名称']}",
                    symbol=row['板块名称']
                ): row['板块名称']
                for _, row in top_industries.iterrows()
            }
            for future in as_completed(future_to_industry):
                industry_name = future_to_industry[future]
                try:
                    constituents_df = future.result()
                    if not constituents_df.empty:
                        print(f"  - 成功获取板块 '{industry_name}' 的成分股。")
                        # 添加板块名称列
                        constituents_df['所属板块'] = industry_name
                        # 确保股票代码列为字符串格式，防止0丢失
                        if '代码' in constituents_df.columns:
                            constituents_df.rename(columns={'代码': '股票代码'}, inplace=True)
                        constituents_df['股票代码'] = constituents_df['股票代码'].astype(str).str.zfill(6)
                        constituents_df['完整股票编码'] = constituents_df['股票代码'].apply(format_stock_code)
                        all_constituents.append(constituents_df)
                    else:
                        print(f"  - 警告：未能获取板块 '{industry_name}' 的成分股数据。")
                except Exception as e:
                    print(f"  - [ERROR] 获取板块 '{industry_name}' 成分股时出错: {e}")

        if all_constituents:
            merged_df = pd.concat(all_constituents, ignore_index=True)
            print(f"  - 已合并所有前板块成分股数据，共 {len(merged_df)} 条。")
            return merged_df
        else:
            print("  - 所有板块成分股数据均获取失败。")
            return pd.DataFrame()

    # >> 新增持续放量数据的获取方法
    def fetch_continuous_volume_increase(self) -> pd.DataFrame:
        """
        获取同花顺-持续放量数据。
        接口: ak.stock_rank_cxfl_ths
        """
        print("\n>>> 正在获取同花顺-持续放量数据...")
        # Note: The file base name should match the name used in processor
        df = self.fetch_with_cache(ak.stock_rank_cxfl_ths, '持续放量')
        return df

    # << 新增持续放量数据的获取方法

    def fetch_hist_data_parallel(self, codes: list, days: int) -> pd.DataFrame:
        """并行获取指定股票代码的历史数据，并缓存到本地文件。"""
        print(f"\n正在为 {len(codes)} 只股票下载 {days} 天的历史数据，使用15个线程并行处理。")
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        start_date_str = start_date.strftime("%Y%m%d")
        end_date_str = end_date.strftime("%Y%m%d")
        if os.path.exists(self.macd_cache_file):
            # 检查缓存是否过期（例如，如果缓存不是今天创建的，则重新下载）
            cache_date = datetime.fromtimestamp(os.path.getmtime(self.macd_cache_file)).strftime("%Y%m%d")
            if cache_date == self.today_str:
                print(f"发现今日历史数据缓存文件，直接加载。")
                return pd.read_csv(self.macd_cache_file, sep='|', encoding='utf-8', dtype={'股票代码': str})
            else:
                print("发现旧的历史数据缓存文件，将重新下载。")

        all_data = []
        # 将代码转换为完整的市场编码
        future_to_code = {}
        with ThreadPoolExecutor(max_workers=15) as executor:
            for code in codes:
                future = executor.submit(
                    ak.stock_zh_a_hist_tx,
                    symbol=format_stock_code(code),
                    start_date=start_date_str,
                    end_date=end_date_str,
                    adjust="hfq"
                )
                future_to_code[future] = code

            for i, future in enumerate(as_completed(future_to_code)):
                code = future_to_code[future]
                try:
                    hist_df = future.result()
                    if hist_df is not None and not hist_df.empty:
                        hist_df['股票代码'] = code
                        # 确保日期是字符串格式，避免excel出错
                        if '日期' in hist_df.columns:
                            hist_df['日期'] = pd.to_datetime(hist_df['日期']).dt.strftime('%Y-%m-%d')
                        all_data.append(hist_df)

                except Exception as e:
                    print(f"[ERROR] 错误：获取 {code} 的历史数据时出错: {e}，已跳过。")
        if all_data:
            merged_df = pd.concat(all_data, ignore_index=True)
            self.save_data_to_txt(merged_df, self.macd_cache_file)
            return merged_df
        print("[WARN] 未能成功下载任何股票的历史数据。")
        return pd.DataFrame()

    # >> NEW: 添加获取单个股票所属行业信息的方法
    def fetch_industry_info(self, code: str) -> Dict[str, str]:
        """为单个股票代码获取所属行业信息 (ak.stock_individual_info_em)。"""
        try:
            # 使用 ak.stock_individual_info_em 获取股票信息
            info_df = ak.stock_individual_info_em(symbol=code)

            # 查找 '行业' 字段的值
            industry = info_df[info_df['项目'] == '所属行业']['值'].iloc[0]
            return {code: industry}
        except Exception:
            # print(f"[WARN] 获取 {code} 行业信息失败: {e}")
            return {code: 'N/A'}

    def get_industry_map_parallel(self, codes: List[str]) -> Dict[str, str]:
        """并行获取多个股票代码的所属行业信息，使用 5 个工作线程。"""
        # --- 核心修改：将 max_workers 显式设置为 5 ---
        MAX_INDUSTRY_WORKERS = 5
        print(f"\n正在并行获取 {len(codes)} 只股票的所属行业信息，使用 {MAX_INDUSTRY_WORKERS} 个线程...")
        
        industry_map = {}
        future_to_code = {}

        # 重新创建 executor 以确保线程池独立且只使用 5 个 workers
        with ThreadPoolExecutor(max_workers=MAX_INDUSTRY_WORKERS) as executor:
            for code in codes:
                future = executor.submit(self.fetch_industry_info, code)
                future_to_code[future] = code

            for i, future in enumerate(as_completed(future_to_code)):
                code = future_to_code[future]
                try:
                    result = future.result()
                    industry_map.update(result)

                    if (i + 1) % 50 == 0 or (i + 1) == len(codes):
                        print(f"  - 进度: 已处理 {i + 1}/{len(codes)} 只股票的行业信息。")
                except Exception:
                    # 失败时已在 fetch_industry_info 中处理，这里只记录
                    industry_map[code] = 'N/A'

        print("所属行业信息获取完成。")
        return industry_map
    # << NEW: 添加获取单个股票所属行业信息的方法


# ==============================================================================
# 数据处理类
# ==============================================================================
class DataProcessor:
    """
    负责对获取的数据进行清洗、合并和技术指标计算。
    """

    def __init__(self, data_fetcher: DataFetcher):
        self.fetcher = data_fetcher
        self.executor = ThreadPoolExecutor(max_workers=self.fetcher.config.MAX_WORKERS)
        self.start_date_for_ta = (datetime.now() - pd.DateOffset(months=6)).strftime("%Y%m%d")
        self.end_date_for_ta = datetime.now().strftime("%Y%m%d")
        self.code_aliases = {'代码': '股票代码', '股票代码': '股票代码', '证券代码': '股票代码'}
        self.name_aliases = {'名称': '股票简称', '股票名称': '股票简称', '股票简称': '股票简称'}
        self.price_aliases = {'最新价': '最新价', '现价': '最新价'}

    def standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """标准化 DataFrame 的列名，确保'股票代码'和'股票简称'等列存在。"""
        if df.empty:
            return df
        found_code_col = False
        for old_name, new_name in self.code_aliases.items():
            if old_name in df.columns:
                df.rename(columns={old_name: new_name}, inplace=True)
                found_code_col = True
                break
        if not found_code_col:
            # print(f"[WARN] 未能在数据中找到股票代码列，原始列名: {df.columns.tolist()}")
            return pd.DataFrame()
        found_name_col = False
        for old_name, new_name in self.name_aliases.items():
            if old_name in df.columns:
                df.rename(columns={old_name: new_name}, inplace=True)
                found_name_col = True
                break
        if not found_name_col and '股票简称' not in df.columns:
            # print(f"[WARN] 未能在数据中找到股票名称列，原始列名: {df.columns.tolist()}")
            pass  # 允许没有股票简称，但可能有股票代码
        found_price_col = False
        for old_name, new_name in self.price_aliases.items():
            if old_name in df.columns:
                df.rename(columns={old_name: new_name}, inplace=True)
                found_price_col = True
                break
        if not found_price_col and '最新价' not in df.columns:
            # print(f"[WARN] 未能在数据中找到价格列，原始列名: {df.columns.tolist()}")
            pass
        return df

    def clean_data(self, df: pd.DataFrame, df_name: str) -> pd.DataFrame:
        """通用数据清洗函数，处理缺失值、重复值并去除ST股，并保存清洗后的数据，删除原始文件。"""
        initial_rows = len(df)

        # 1. 构造原始文件路径，用于删除
        today_str = self.fetcher.today_str
        original_file_name = f"{df_name}_{today_str}.txt"
        original_file_path = os.path.join(self.fetcher.config.TEMP_DATA_DIRECTORY, original_file_name)

        # 2. 执行清洗和标准化
        df = self.standardize_columns(df)
        if df.empty or '股票代码' not in df.columns:
            print(f"[WARN] {df_name} 数据标准化失败或为空，跳过清洗。")
            return pd.DataFrame()

        df.dropna(subset=['股票代码'], inplace=True)
        df.drop_duplicates(subset=['股票代码'], inplace=True)
        df['股票代码'] = df['股票代码'].astype(str).str.zfill(6)

        # 优化：在清洗前添加一个临时的股票简称列，以防缺失，方便ST过滤
        if '股票简称' not in df.columns:
            df['股票简称'] = df['股票代码'].astype(str)  # 临时使用代码

        cleaned_df = df[~df['股票简称'].str.contains('ST|st|退市', case=False, na=False)].copy()

        # 恢复原始的股票简称列（如果临时添加了）
        if '股票简称' in df.columns and cleaned_df.columns.tolist()[
            -1] == '股票简称' and '股票简称' not in self.name_aliases.values():
            pass

        final_rows = len(cleaned_df)
        print(f"{df_name} 清洗完成。清洗前：{initial_rows} 条，清洗后：{final_rows} 条。")

        # 3. 构造并保存清洗后的文件
        cleaned_file_name = f"{df_name}_经清洗_{today_str}.txt"
        cleaned_file_path = os.path.join(self.fetcher.config.TEMP_DATA_DIRECTORY, cleaned_file_name)
        self.fetcher.save_data_to_txt(cleaned_df, cleaned_file_path)

        # 4. 删除原始文件
        try:
            # 只有当原始文件存在（即未命中清洗缓存）时才删除
            if os.path.exists(original_file_path):
                os.remove(original_file_path)
                print(f"已删除原始临时文件: {original_file_name}")
        except Exception as e:
            print(f"[WARN] 警告：删除原始文件 {original_file_name} 失败: {e}")

        return cleaned_df

    def process_profit_data(self, df: pd.DataFrame, min_rating: int = 2) -> pd.DataFrame:
        df = self.clean_data(df, "主力研报盈利预测")
        if df.empty:
            return pd.DataFrame()
        df['机构投资评级(近六个月)-买入'] = pd.to_numeric(df['机构投资评级(近六个月)-买入'], errors='coerce')
        df = df[df['机构投资评级(近六个月)-买入'] >= min_rating].copy()
        df['完整股票编码'] = df['股票代码'].apply(format_stock_code)
        print(f"研报数据过滤完成，符合条件的股票数量: {len(df)}")
        return df

    def process_main_report_sheet(self, profit_df: pd.DataFrame, spot_df: pd.DataFrame) -> pd.DataFrame:
        """生成“主力研报筛选” Sheet 的数据，不再包含股票链接、最新价、序号。"""
        if profit_df.empty:
            print("[WARN] 研报数据为空，无法生成主力研报筛选表。")
            return pd.DataFrame()

        # 1. 使用 profit_df 的拷贝作为基础，不再合并 spot_df 中的 '最新价'
        final_df = profit_df.copy()

        # 2. 移除不再需要的列：'完整股票编码'（曾用于生成股票链接）
        cols_to_drop = ['完整股票编码']

        for col in cols_to_drop:
            if col in final_df.columns:
                final_df.drop(columns=[col], inplace=True)

        # 3. 确保 '股票代码' 和 '股票简称' 在最前面
        leading_cols = ['股票代码', '股票简称']
        existing_leading_cols = [col for col in leading_cols if col in final_df.columns]
        other_cols = [col for col in final_df.columns if col not in existing_leading_cols]

        # 返回最终筛选后的 DataFrame
        return final_df[existing_leading_cols + other_cols]

    def process_spot_data(self, spot_data_all: pd.DataFrame, filtered_codes_df: pd.DataFrame) -> pd.DataFrame:
        """处理实时行情数据，并确保价格列名为'当前价格'。"""
        # --- 核心修改：移除备用接口的获取和合并逻辑 ---
        # 仅处理传入的主接口数据
        spot_data_all = self.clean_data(spot_data_all, "A股实时行情")

        if spot_data_all.empty or filtered_codes_df.empty:
            # 如果是空DF，则只返回 spot_data_all 的清洗结果
            return spot_data_all

        # 核心修复：确保价格列名为'当前价格'，以便在 find_recommended_stocks_with_score 中区分使用
        if '最新价' in spot_data_all.columns:
            spot_data_all.rename(columns={'最新价': '当前价格'}, inplace=True)
        elif '现价' in spot_data_all.columns:
            spot_data_all.rename(columns={'现价': '当前价格'}, inplace=True)

        # 确保合并后保留'股票代码'和'当前价格'
        # 注意：这里不需要 filtered_codes_df，因为 spot_data_all 是 A股全量的实时行情，直接返回即可
        return spot_data_all[['股票代码', '股票简称', '当前价格']].drop_duplicates(subset=['股票代码'])
        # -----------------------------------------------

    def process_financial_abstract(self, df: pd.DataFrame) -> pd.DataFrame:
        """处理财务摘要数据，进行清洗和格式化。"""
        return self.clean_data(df, "财务摘要数据")

    def process_market_fund_flow(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        处理市场资金流向数据 (5日排行)。
        """
        # 注意：clean_data 会将价格列（如'最新价'）标准化为 '最新价'
        df = self.clean_data(df, "市场资金流向")
        if df.empty:
            print("警告：未能获取市场资金流向数据。")
            return pd.DataFrame()
        # 按照“流入资金”字段倒序排序
        if '流入资金' in df.columns:
            df['流入资金'] = pd.
